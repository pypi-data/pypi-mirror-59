# -*- coding: utf-8 -*-
# @Time    : 2019/1/13/013 20:00 下午
# @Author  : LQX
# @Email   : qxsoftware@163.com
# @File    : save_load.py
# @Software: PyCharm

import os
import shutil
import time
import torch as t
from warnings import warn
from torch import save, load
from torch.nn import Module
from torch.optim import Optimizer
from torchservant._basic import *
from torchservant.cfgenator.config import BasicConfig


def make_ckpt_to(path, model, start_time=None, epoch=None, loss_val=None, train_score=None, val_score=None,
                 optimizer=None, errlevel=1):
    try:
        ckpt = {'epoch': epoch,
                'start_time': start_time,
                'save_time': time.time(),
                'loss_val': loss_val,
                'train_score': train_score,
                'val_score': val_score,
                'model_state_dict': model.state_dict()}
        if optimizer is not None and hasattr(optimizer, "state_dict"):
            ckpt['optimizer_state_dict'] = optimizer.state_dict()
        else:
            ckpt['optimizer_state_dict'] = None
        save(ckpt, path)
    except Exception as e:
        handle_err(e, errlevel)


def make_checkpoint(config, epoch, start_time, loss_val, train_score, val_score, model, optimizer=None):
    # type:(BasicConfig,int,float,float,float,float,Module,Optimizer)->None
    """
    generate temporary training process data for resuming by resume_checkpoint()
    """
    make_ckpt_to(config.temp_ckpt_path, model, start_time, epoch, loss_val, train_score, val_score, optimizer,
                 config.errlevel)
    with open(config.train_record_file, 'a+') as f:
        elapsed_time = time.time() - start_time,
        record = config.__record_dict__.format(config.init_time, epoch, start_time, elapsed_time, loss_val, train_score,
                                               val_score)
        f.write(record + '\n')


def resume_ckpt_from(path, model, optimizer=None, printproc=True, errlevel=1):
    if os.path.exists(path):
        try:
            ckpt = load(path)
            epoch = ckpt['epoch']
            start_time = ckpt['start_time']
            save_time = ckpt['save_time']
            loss_val = ckpt['loss_val']
            train_score = ckpt['train_score']
            val_score = ckpt['val_score']
            model_sdict = ckpt['model_sdict']
            optimizer_sdict = ckpt['optimizer_state_dict']
            if model_sdict is None:
                raise RuntimeError("Bad model_state_dict in {}".format(path))
            else:
                model.load_state_dict(model_sdict)
            if optimizer_sdict is None:
                handle_err("None optimizer_state_dict!", errlevel)
            else:
                optimizer.load_state_dict(optimizer_sdict)
            if printproc:
                msg = "Resume checkpoint from {}".format(path)
                if epoch is not None:
                    msg += ",last epoch:{}".format(epoch)
                if start_time is not None and save_time is not None:
                    msg += ",from {} to {}".format(start_time, save_time)
                if loss_val is not None:
                    msg += ",loss:{}".format(loss_val)
                if train_score is not None:
                    msg += ",train score:{}".format(train_score)
                if val_score is not None:
                    msg += ",val score:{}".format(val_score)
                print(msg)
            return True, epoch, start_time, save_time, loss_val, train_score, val_score
        except Exception as e:
            handle_err("Can't resume checkpoint from {}. Error message:{}".format(path, e), errlevel)
            return False, None, None, None, None, None, None
    else:
        handle_err("File not exist in {}".format(path), errlevel)
        return False, None, None, None, None, None, None


def resume_checkpoint(config: BasicConfig, model: Module, optimizer: Optimizer = None, printproc=True) -> int:
    """
    resume training process data from config.logs which generated by make_checkpoint()
    :return number of last epoch
    """
    # Find last ckpt file path
    last_epoch = -1
    last_ckpt_path = config.temp_ckpt_path
    if os.path.exists(config.train_record_file):
        try:
            with open(config.train_record_file, 'r') as f:
                last = f.readlines()[-1]
                import json
                info = json.loads(last)
                last_epoch = int(info["epoch"])
                last_init = str(info["init"])
                if not os.path.exists(last_ckpt_path):
                    last_ckpt_path = last_ckpt_path.replace(config.init_time, last_init)
            if printproc:
                print("Continue train from last epoch %d" % last_epoch)
        except:
            handle_err("Invalid train_record_file:{}".format(config.train_record_file), config.errlevel)
            if config.errlevel == 1:
                warn("Rename {} to {}".format(config.train_record_file, config.train_record_file + '.badfile'))
            os.rename(config.train_record_file, config.train_record_file + '.badfile')
            handle_err("Can't get last_epoch value, {} will be returned".format(last_epoch), config.errlevel)
    # resume from last_ckpt_path
    if os.path.exists(last_ckpt_path):
        ret, epoch, _, _, _, _, _ = resume_ckpt_from(last_ckpt_path, model, optimizer, printproc, config.errlevel)
        if ret is False and config.errlevel == 1:
            warn("Move invalid temp {} weights file from {} to {}".format(type(model), last_ckpt_path,
                                                                          last_ckpt_path + '.badfile'))
            os.rename(last_ckpt_path, last_ckpt_path + '.badfile')
        if epoch != last_epoch:
            handle_err("Different epoch value between ckpt file ({}) and train_record_file ({})!".
                       format(last_epoch, epoch), config.errlevel)
        if printproc:
            print("Resumed weight checkpoint from {}".format(last_ckpt_path))
    return last_epoch


def save_model_to(path, model):
    try:
        # remove the nn.DataParallel layer(s) in case of complex situation when loading to CPU.
        while isinstance(model, t.nn.DataParallel):
            model = list(model.children())[0]
        t.save(model.state_dict(), path)
        return True
    except Exception as e:
        handle_err("Failed to save model because {}".format(e))
        return False


def save_model_and_dump_history(config: BasicConfig, model: Module,
                                last_epoch=None, loss_val=None, val_score=None, optimizer=None, printproc=True):
    try:
        save_path = config.__format_path__(config.weight_save_path, last_epoch, loss_val, val_score, optimizer)
        if save_model_to(save_path, model) and printproc:
            print("Model saved into " + config.weight_save_path)
        else:
            raise RuntimeError
    except Exception as e:
        handle_err("Failed to save model, message:{}\ntemp_chpt_path:{}".format(e, config.temp_ckpt_path),
                   config.errlevel)
    # dump local temporary files into history directory
    try:
        os.remove(config.temp_ckpt_path)
        dist = os.path.join(config.history_root, config.log_root + config.init_time)
        shutil.move(config.log_root, dist)
        if printproc:
            print("Correlation logs has been moved into ", dist)
    except Exception as e:
        handle_err("Failed to dump history because {}, check temp files in {}".format(e, config.log_root),
                   config.errlevel)
