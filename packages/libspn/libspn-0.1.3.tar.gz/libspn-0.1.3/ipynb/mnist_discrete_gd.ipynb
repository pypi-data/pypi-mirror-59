{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminative learning for discrete MNIST data using randomly structured SPNs\n",
    "This notebook shows how to build a randomly structured SPN and train it to classify digits using a TensorFlow optimizer on binarized MNIST data.\n",
    "\n",
    "### Setting up the imports and preparing the data\n",
    "We load the data from `tf.keras.datasets`. Preprocessing consists of flattening and binarization of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import libspn as spn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from libspn.examples.utils.dataiterator import DataIterator\n",
    "\n",
    "# Load\n",
    "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "def binarize(x):\n",
    "    return np.where(np.greater(x / 255., 0.25), 1.0, 0.0)\n",
    "\n",
    "def flatten(x):\n",
    "    return x.reshape(-1, np.prod(x.shape[1:]))\n",
    "\n",
    "def preprocess(x, y):\n",
    "    return binarize(flatten(x)).astype(int), np.expand_dims(y, axis=1)\n",
    "\n",
    "# Preprocess\n",
    "train_x, train_y = preprocess(train_x, train_y)\n",
    "test_x, test_y = preprocess(test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the hyperparameters\n",
    "Some hyperparameters for the SPN. \n",
    "- `num_subsets` is used for the `DenseSPNGenerator`. This corresponds to the number of variable subsets joined by product nodes in the SPN.\n",
    "- `num_mixtures` is used for the `DenseSPNGenerator`. This corresponds to the number of sum nodes per scope.\n",
    "- `num_decomps` is used for the `DenseSPNGenerator`. This corresponds to the number of decompositions generated at each level of products from top-down.\n",
    "- `num_vars` corresponds to the number of input variables (the number of pixels in the case of MNIST).\n",
    "- `balanced` is used for the `DenseSPNGenerator`. If true, then the generated SPN will have balanced subsets and will consequently be a balanced tree.\n",
    "- `input_dist` is the input distribution (the first product/sum layer in the SPN). `spn.DenseSPNGenerator.InputDist.RAW` corresponds to raw indicators being joined (so first layer is a product layer). `spn.DenseSPNGenerator.InputDist.MIXTURE` would correspond to a sums on top of each indicator.\n",
    "- `num_leaf_values` is the number of unique discrete values in the leaf distribution (2 since data is binary).\n",
    "- `inference_type` determines the kind of forward inference where `spn.InferenceType.MARGINAL` corresponds to sum nodes marginalizing their inputs. `spn.InferenceType.MPE` would correspond to having max nodes instead.\n",
    "- `beta1` corresponds to the `\\beta_1` parameter of the Adam optimizer\n",
    "- `beta2` corresponds to the `\\beta_2` parameter of the Adam optimizer\n",
    "- `learning_rate` is the learning rate for the Adam optimizer\n",
    "- `num_classes`, `batch_size` and `num_epochs` should be obvious:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of variable subsets that a product joins\n",
    "num_subsets = 2\n",
    "# Number of sums per scope\n",
    "num_mixtures = 4\n",
    "# Number of decompositions per product layer\n",
    "num_decomps = 1\n",
    "# Generate balanced subsets -> balanced tree\n",
    "balanced = True\n",
    "# Number of variables\n",
    "num_vars = train_x.shape[1]\n",
    "# Input distribution. Raw corresponds to first layer being product that \n",
    "# takes raw indicators\n",
    "input_dist = spn.DenseSPNGenerator.InputDist.RAW\n",
    "# Number of different values at leaf (binary here, so 2)\n",
    "num_leaf_values = 2\n",
    "# Initial value for path count accumulators\n",
    "initial_accum_value = 0.1\n",
    "# Inference type (can also be spn.InferenceType.MPE) where \n",
    "# sum nodes are turned into max nodes\n",
    "inference_type = spn.InferenceType.MARGINAL\n",
    "# Adam optimizer parameters\n",
    "beta1 = 0.9\n",
    "beta2 = 0.9\n",
    "learning_rate = 5e-4\n",
    "# Other params\n",
    "num_classes = 10\n",
    "batch_size = 32\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the SPN\n",
    "Our SPN consists of binary leaf indicators, a dense SPN per class and a root node connecting the 10 class-wise sub-SPNs. We also add an indicator node to the root node to model the latent class variable. Finally, we generate `Weight` nodes for the full SPN by using `spn.generate_weights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPN depth: 21\n",
      "Number of products layers: 10\n",
      "Number of sums layers: 10\n"
     ]
    }
   ],
   "source": [
    "# Leaf nodes\n",
    "leaf_indicators = spn.IndicatorLeaf(num_vals=num_leaf_values, num_vars=num_vars)\n",
    "\n",
    "# Generates densely connected random SPNs\n",
    "dense_generator = spn.DenseSPNGenerator(\n",
    "    num_subsets=num_subsets, num_mixtures=num_mixtures, num_decomps=num_decomps, \n",
    "    balanced=balanced, input_dist=input_dist, \n",
    "    node_type=spn.DenseSPNGenerator.NodeType.BLOCK)\n",
    "\n",
    "# Generate a dense SPN for each class\n",
    "class_roots = [dense_generator.generate(leaf_indicators) for _ in range(num_classes)]\n",
    "\n",
    "# Connect sub-SPNs to a root\n",
    "root = spn.convert_to_layer_nodes(spn.Sum(*class_roots, name=\"RootSum\"))\n",
    "\n",
    "# Add an IVs node to the root as a latent class variable\n",
    "class_indicators = root.generate_latent_indicators()\n",
    "\n",
    "# Generate the weights for the SPN rooted at `root`\n",
    "spn.generate_weights(root)\n",
    "\n",
    "print(\"SPN depth: {}\".format(root.get_depth()))\n",
    "print(\"Number of products layers: {}\".format(root.get_num_nodes(node_type=spn.ProductsLayer)))\n",
    "print(\"Number of sums layers: {}\".format(root.get_num_nodes(node_type=spn.SumsLayer)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the TensorFlow graph\n",
    "Now that we have defined the SPN graph we can declare the TensorFlow operations needed for training and evaluation. The `MPEState` class can be used to find the MPE state of any node in the graph. In this case we might be interested in finding the most likely class based on the evidence elsewhere. This corresponds to the MPE state of `class_indicators`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Op for initializing all weights\n",
    "weight_init_op = spn.initialize_weights(root)\n",
    "# Op for getting the log probability of the root\n",
    "root_log_prob = root.get_log_value(inference_type=inference_type)\n",
    "\n",
    "# Set up ops for discriminative GD learning\n",
    "gd_learning = spn.GDLearning(\n",
    "    root=root, learning_task_type=spn.LearningTaskType.SUPERVISED,\n",
    "    learning_method=spn.LearningMethodType.DISCRIMINATIVE)\n",
    "optimizer = tf.train.AdamOptimizer(beta1=0.95, beta2=0.95)\n",
    "\n",
    "# Use post_gradients_ops = True to also normalize weights (and clip Gaussian variance)\n",
    "gd_update_op = gd_learning.learn(optimizer=optimizer, post_gradient_ops=True)\n",
    "\n",
    "# Compute predictions and matches\n",
    "mpe_state = spn.MPEState()\n",
    "root_marginalized = spn.Sum(*root.values, weights=root.weights)\n",
    "marginalized_ivs = root_marginalized.generate_latent_indicators(\n",
    "    feed=-tf.ones_like(class_indicators.feed)) \n",
    "predictions, = mpe_state.get_state(root_marginalized, marginalized_ivs)\n",
    "with tf.name_scope(\"MatchPredictionsAndTarget\"):\n",
    "    match_op = tf.equal(tf.to_int64(predictions), tf.to_int64(class_indicators.feed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the SPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 313/313 [00:10<00:00, 29.41it/s, Accuracy=0.19]\n",
      "Training:   0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training test accuracy = 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1875/1875 [01:48<00:00, 12.78it/s, Accuracy=0.91]\n",
      "Testing: 100%|██████████| 313/313 [00:10<00:00, 31.06it/s, Accuracy=1.00]\n",
      "Training:   0%|          | 2/1875 [00:00<01:46, 17.52it/s, Accuracy=0.88]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train accuracy = 0.90, test accuracy = 0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 332/1875 [00:18<01:27, 17.70it/s, Accuracy=0.91]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4dd17e613b5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             batch_matches, _ = sess.run(\n\u001b[0;32m---> 27\u001b[0;31m                 [match_op, gd_update_op], fd(batch_x, batch_y))\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mmatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_matches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAccuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"{:.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_matches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 332/1875 [00:30<02:19, 11.06it/s, Accuracy=0.91]"
     ]
    }
   ],
   "source": [
    "# Set up some convenient iterators\n",
    "train_iterator = DataIterator([train_x, train_y], batch_size=batch_size)\n",
    "test_iterator = DataIterator([test_x, test_y], batch_size=batch_size)\n",
    "\n",
    "def fd(x, y):\n",
    "    return {leaf_indicators: x, class_indicators: y}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize things\n",
    "    sess.run([tf.global_variables_initializer(), weight_init_op])\n",
    "    \n",
    "    # Do one run for test likelihoods\n",
    "    matches = []\n",
    "    for batch_x, batch_y in test_iterator.iter_epoch(\"Testing\"):\n",
    "        batch_matches = sess.run(match_op, fd(batch_x, batch_y))\n",
    "        matches.extend(batch_matches.ravel())\n",
    "        test_iterator.display_progress(Accuracy=\"{:.2f}\".format(np.mean(batch_matches)))\n",
    "    mean_test_accuracy = np.mean(matches)\n",
    "    \n",
    "    print(\"Before training test accuracy = {:.2f}\".format(mean_test_accuracy))                              \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train\n",
    "        matches = []\n",
    "        for batch_x, batch_y in train_iterator.iter_epoch(\"Training\"):\n",
    "            batch_matches, _ = sess.run(\n",
    "                [match_op, gd_update_op], fd(batch_x, batch_y))\n",
    "            matches.extend(batch_matches.ravel())\n",
    "            train_iterator.display_progress(Accuracy=\"{:.2f}\".format(np.mean(batch_matches)))\n",
    "        mean_train_accuracy = np.mean(matches)\n",
    "        \n",
    "        # Test\n",
    "        matches = []\n",
    "        for batch_x, batch_y in test_iterator.iter_epoch(\"Testing\"):\n",
    "            batch_matches = sess.run(match_op, fd(batch_x, batch_y))\n",
    "            matches.extend(batch_matches.ravel())\n",
    "            test_iterator.display_progress(Accuracy=\"{:.2f}\".format(np.mean(batch_matches)))\n",
    "        mean_test_accuracy = np.mean(matches)\n",
    "        \n",
    "        # Report\n",
    "        print(\"Epoch {}, train accuracy = {:.2f}, test accuracy = {:.2f}\".format(\n",
    "            epoch, mean_train_accuracy, mean_test_accuracy))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
