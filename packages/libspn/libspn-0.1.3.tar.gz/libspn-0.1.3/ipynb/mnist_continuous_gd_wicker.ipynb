{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wicker Convolutional SPNs for continuous MNIST data\n",
    "This notebook shows how to build Wicker Convolutional SPNs (WCSPNs) and use them to classifiy digits with the MNIST dataset.\n",
    "\n",
    "### Setting up the imports and preparing the data\n",
    "We load the data from `tf.keras.datasets`. Preprocessing consists of flattening and binarization of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import libspn as spn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from libspn.examples.utils.dataiterator import DataIterator\n",
    "\n",
    "# Load\n",
    "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "def binarize(x):\n",
    "    return x / 255.\n",
    "\n",
    "def flatten(x):\n",
    "    return x.reshape(-1, np.prod(x.shape[1:]))\n",
    "\n",
    "def preprocess(x, y):\n",
    "    return binarize(flatten(x)), np.expand_dims(y, axis=1)\n",
    "\n",
    "# Preprocess\n",
    "train_x, train_y = preprocess(train_x, train_y)\n",
    "test_x, test_y = preprocess(test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the hyperparameters\n",
    "Some hyperparameters for the SPN. \n",
    "- `num_vars` corresponds to the number of input variables (the number of pixels in the case of MNIST).\n",
    "- `num_leaf_components` is the number of distribution components in the normal leafs\n",
    "- `inference_type` determines the kind of forward inference where `spn.InferenceType.MARGINAL` corresponds to sum nodes marginalizing their inputs. `spn.InferenceType.MPE` would correspond to having max nodes instead.\n",
    "- `learning_rate` is the learning rate for the Adam optimizer\n",
    "- `scale_init`, initial scale value for the `NormalLeaf` node. This parameter greatly determines the stability of the training process\n",
    "- `num_classes`, `batch_size` and `num_epochs` should be obvious:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of variables\n",
    "num_vars = train_x.shape[1]\n",
    "# Number of different values at leaf (binary here, so 2)\n",
    "num_leaf_components = 4\n",
    "# Inference type (can also be spn.InferenceType.MPE) where \n",
    "# sum nodes are turned into max nodes\n",
    "inference_type = spn.InferenceType.MARGINAL\n",
    "# Adam optimizer parameters\n",
    "learning_rate = 1e-2\n",
    "# Scale init\n",
    "scale_init = 0.1\n",
    "# Other params\n",
    "num_classes = 10\n",
    "batch_size = 32\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the SPN\n",
    "Our SPN consists of a leaf node with normal distributions followed by spatial products and sums. A `ConvProducts` node will generate all possible permutations of the child channels (if possible). A `ConvProductsDepthwise` will use the subset of permutations that corresponds to depthwise convolutions. Products are in fact implemented as convolutions, since multiplications become sums in the log-space. `LocalSums` consist of sums that are applied 'locally', without weight sharing, so they are in a sense comparable to `LocallyConnected` layers in `Keras`.\n",
    "\n",
    "Note that after two non-overlapping products (with kernel sizes of $2\\times 2$ and strides of $2\\times 2$), we have a 'wicker' stack where we use `'full'` padding and exponentially increasing dilation rates.\n",
    "\n",
    "Finally, we apply a `ConvProductDepthwise` layer with `'wicker_top'` padding to get scopes which include all variables at the final layer. This layer can then be connected to class roots, which are in turn connected to a single root node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# Leaf nodes\n",
    "\n",
    "normal_leafs = spn.NormalLeaf(\n",
    "    num_components=num_leaf_components, num_vars=num_vars, \n",
    "    trainable_scale=False, trainable_loc=True, scale_init=scale_init)\n",
    "\n",
    "# Twice non-overlapping convolutions\n",
    "x = spn.ConvProducts(normal_leafs, num_channels=32, padding='valid', kernel_size=2, strides=2, spatial_dim_sizes=[28, 28])\n",
    "x = spn.LocalSums(x, num_channels=32)\n",
    "x = spn.ConvProductsDepthwise(x, padding='valid', kernel_size=2, strides=2)\n",
    "x = spn.LocalSums(x, num_channels=32)\n",
    "\n",
    "# Make a wicker stack\n",
    "stack_size = int(np.ceil(np.log2(28 // 4)))\n",
    "for i in range(stack_size):\n",
    "    dilation_rate = 2 ** i\n",
    "    x = spn.ConvProductsDepthwise(\n",
    "        x, padding='full', kernel_size=2, strides=1, dilation_rate=dilation_rate)\n",
    "    x = spn.LocalSums(x, num_channels=64)\n",
    "# Create final layer of products\n",
    "full_scope_prod = spn.ConvProductsDepthwise(\n",
    "    x, padding='wicker_top', kernel_size=2, strides=1, dilation_rate=2 ** stack_size)\n",
    "class_roots = spn.ParallelSums(full_scope_prod, num_sums=num_classes)\n",
    "root = spn.Sum(class_roots)\n",
    "\n",
    "# Add a IndicatorLeaf node to the root as a latent class variable\n",
    "class_indicators = root.generate_latent_indicators()\n",
    "\n",
    "# Generate the weights for the SPN rooted at `root`\n",
    "spn.generate_weights(root, log=True, initializer=tf.initializers.random_uniform())\n",
    "\n",
    "print(\"SPN depth: {}\".format(root.get_depth()))\n",
    "print(\"Number of products layers: {}\".format(root.get_num_nodes(node_type=spn.ConvProducts)))\n",
    "print(\"Number of sums layers: {}\".format(root.get_num_nodes(node_type=spn.LocalSums)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the TensorFlow graph\n",
    "Now that we have defined the SPN graph we can declare the TensorFlow operations needed for training and evaluation. The `MPEState` class can be used to find the MPE state of any node in the graph. In this case we might be interested in finding the most likely class based on the evidence elsewhere. This corresponds to the MPE state of `class_indicators`.\n",
    "\n",
    "Note that for the gradient optimizer we use `AMSGrad`, which usually yields reasonable results much faster than Adam. Admittedly, more time needs to be spent on the interdependencies of parameters (e.g. `scale_init`) affect training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libspn.examples.convspn.amsgrad import AMSGrad\n",
    "\n",
    "# Op for initializing all weights\n",
    "weight_init_op = spn.initialize_weights(root)\n",
    "# Op for getting the log probability of the root\n",
    "root_log_prob = root.get_log_value(inference_type=inference_type)\n",
    "\n",
    "# Set up ops for discriminative GD learning\n",
    "gd_learning = spn.GDLearning(\n",
    "    root=root, learning_task_type=spn.LearningTaskType.SUPERVISED,\n",
    "    learning_method=spn.LearningMethodType.DISCRIMINATIVE)\n",
    "optimizer = AMSGrad(learning_rate=learning_rate)\n",
    "\n",
    "# Use post_gradients_ops = True to also normalize weights (and clip Gaussian variance)\n",
    "gd_update_op = gd_learning.learn(optimizer=optimizer, post_gradient_ops=True)\n",
    "\n",
    "# Compute predictions and matches\n",
    "mpe_state = spn.MPEState()\n",
    "root_marginalized = spn.Sum(root.values[0], weights=root.weights)\n",
    "marginalized_ivs = root_marginalized.generate_latent_indicators(\n",
    "    feed=-tf.ones_like(class_indicators.feed)) \n",
    "predictions, = mpe_state.get_state(root_marginalized, marginalized_ivs)\n",
    "with tf.name_scope(\"MatchPredictionsAndTarget\"):\n",
    "    match_op = tf.equal(tf.to_int64(predictions), tf.to_int64(class_indicators.feed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the SPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some convenient iterators\n",
    "train_iterator = DataIterator([train_x, train_y], batch_size=batch_size)\n",
    "test_iterator = DataIterator([test_x, test_y], batch_size=batch_size)\n",
    "\n",
    "def fd(x, y):\n",
    "    return {normal_leafs: x, class_indicators: y}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize things\n",
    "    sess.run([tf.global_variables_initializer(), weight_init_op])\n",
    "    \n",
    "    # Do one run for test likelihoods\n",
    "    matches = []\n",
    "    for batch_x, batch_y in test_iterator.iter_epoch(\"Testing\"):\n",
    "        batch_matches = sess.run(match_op, fd(batch_x, batch_y))\n",
    "        matches.extend(batch_matches.ravel())\n",
    "        test_iterator.display_progress(Accuracy=\"{:.2f}\".format(np.mean(batch_matches)))\n",
    "    mean_test_accuracy = np.mean(matches)\n",
    "    \n",
    "    print(\"Before training test accuracy = {:.2f}\".format(mean_test_accuracy))                              \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train\n",
    "        matches = []\n",
    "        for batch_x, batch_y in train_iterator.iter_epoch(\"Training\"):\n",
    "            batch_matches, _ = sess.run(\n",
    "                [match_op, gd_update_op], fd(batch_x, batch_y))\n",
    "            matches.extend(batch_matches.ravel())\n",
    "            train_iterator.display_progress(Accuracy=\"{:.2f}\".format(np.mean(batch_matches)))\n",
    "        mean_train_accuracy = np.mean(matches)\n",
    "        \n",
    "        # Test\n",
    "        matches = []\n",
    "        for batch_x, batch_y in test_iterator.iter_epoch(\"Testing\"):\n",
    "            batch_matches = sess.run(match_op, fd(batch_x, batch_y))\n",
    "            matches.extend(batch_matches.ravel())\n",
    "            test_iterator.display_progress(Accuracy=\"{:.2f}\".format(np.mean(batch_matches)))\n",
    "        mean_test_accuracy = np.mean(matches)\n",
    "        \n",
    "        # Report\n",
    "        print(\"Epoch {}, train accuracy = {:.2f}, test accuracy = {:.2f}\".format(\n",
    "            epoch, mean_train_accuracy, mean_test_accuracy))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
